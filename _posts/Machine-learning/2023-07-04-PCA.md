---
title:  "PCA"
categories: ML
tag: [Scikit-learn, Python, Machine-Learning]
author_profile: false
typora-root-url: ../
search: true
use_math: true
---

# Eigenvectors and Eigenvalues

How can we handle this trade-off between simplicity and the amount of information? The answer to this question is the result of the Principal Components Analysis (PCA).

Principal components can be geometrically seen as the directions of high-dimensional data, which **capture the maximum amount of variance** and project it onto a smaller dimensional subspace while keeping most of the information.

- The first principal component accounts for the largest possible variance; the second component will, intuitively, account for the second largest variance (under one condition: it has to be uncorrelated with the first principal component), and so forth.

![image-20230704191049353](/images/2023-07-04-PCA/image-20230704191049353.png)

- As you can see, the covariance matrix defines both the spread (variance) and the orientation (covariance) of our data.
- The vector will point into the direction of the larger spread of data, the number will be equal to the spread (variance) of that direction.

![image-20230704191340240](/images/2023-07-04-PCA/image-20230704191340240.png)

- The direction in green is the **eigenvector**, and it has a corresponding value, called **eigenvalue**, which describes its magnitude.

![image-20230704191532389](/images/2023-07-04-PCA/image-20230704191532389.png)

-  Each eigenvector has a correspondent eigenvalue.
- Now, if we consider our matrix Σ and collect all the corresponding eigenvectors into a matrix V (where the number of columns, which are the eigenvectors, will be equal to the number of rows of Σ), we will obtain something like that:
- ![image-20230704191745894](/images/2023-07-04-PCA/image-20230704191745894.png)
- ![image-20230704191821021](/images/2023-07-04-PCA/image-20230704191821021.png)
- If we sort our eigenvectors in descending order with respect to their eigenvalues, we will have that the first eigenvector accounts for the largest spread among data, the second one for the second largest spread, and so forth (under the condition that all these new directions, which describe a new space, are independent hence orthogonal among each other).

![image-20230704192113345](/images/2023-07-04-PCA/image-20230704192113345.png)

![image-20230704192130742](/images/2023-07-04-PCA/image-20230704192130742.png)